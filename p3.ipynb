{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BXWYbiiNN1hT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex\u003e=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["import numpy as np \n","import pandas as pd \n","%matplotlib inline\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import re\n","import seaborn as sns\n","from IPython.display import display\n","pd.options.mode.chained_assignment = None\n","import matplotlib\n","matplotlib.style.use('ggplot')\n","!pip install nltk\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","tweets = pd.read_csv('/content/drive/My Drive/dataset2.csv', nrows=500,encoding = \"ISO-8859-1\")\n","\n","!pip install vadersentiment\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","display(tweets.head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIFjxnY3N1iK"},"outputs":[],"source":["import re\n","\n","tweets['text_new'] = ''\n","tweets['tweetos'] = '' \n","\n","\n","for i in range(len(tweets['text'][:500])):\n","    try:\n","        tweets['tweetos'][i] = tweets['text'].str.split(':')[i][0]\n","    except AttributeError:    \n","        tweets['tweetos'][i] = 'other'\n","\n","\n","for i in range(len(tweets['text'][:500])):\n","    if tweets['tweetos'].str.contains('RT @')[i]  == False:\n","        tweets['tweetos'][i] = 'other'\n","\n","\n","for i in range(len(tweets['text'][:500])):\n","    m = re.search('(?\u003c=:)(.*)', tweets['text'][i])\n","    if tweets['text'].str.contains('RT @')[i]  == True:\n","        try:\n","            tweets['text_new'][i]=m.group(0)\n","        except AttributeError:\n","            tweets['text_new'][i]=tweets['text'][i] \n","    else:       \n","        tweets['text_new'][i] =  tweets['text'][i]       "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9W83woKlN1iM"},"outputs":[],"source":["from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","\n","def wordcloud_by_province(tweets):\n","    stopwords = set(STOPWORDS)\n","    stopwords.add(\"https\")\n","    stopwords.add(\"00A0\")\n","    stopwords.add(\"00BD\")\n","    stopwords.add(\"00B8\")\n","    stopwords.add(\"ed\")\n","    stopwords.add(\"happy\")\n","    stopwords.add(\"Sad\")\n","   \n","    stopwords.add(\"lakh\")\n","    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords,random_state = 2016).generate(\" \".join([i for i in tweets['text_new'].str.upper()]))\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.title(\"Word cloud\")\n","\n","wordcloud_by_province(tweets)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yog0NkC4N1iO"},"outputs":[],"source":["print(tweets['text'].describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZmPt6vUN1iQ"},"outputs":[],"source":["tweets.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCW25UTFN1iR"},"outputs":[],"source":["tweets['nb_words'] = 0\n","for i in range(len(tweets['text'])):\n","    tweets['nb_words'][i] = len(tweets['text'][i].split(' '))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIGGraY4N1iT"},"outputs":[],"source":["t=tweets['Sentiment'].value_counts()\n","t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtsST_0TN1iV"},"outputs":[],"source":["t.sort_values().plot(kind = 'barh')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAmzAUT1N1iW"},"outputs":[],"source":["def get_stop_words(s, n):\n","\t\n","\tfrom collections import Counter\n","\tl = get_corpus(s)\n","\tl = [x for x in Counter(l).most_common(n)]\n","\treturn l\n","\n","def get_corpus(s):\n","\t\n","\tl = []\n","\ts.map(lambda x: l.extend(x))\n","\treturn l\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IAU7abS1N1iX"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","tweets['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in tweets['text_new']]       \n","\n","vectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=10,stop_words='english',use_idf=True)\n","X = vectorizer.fit_transform(tweets['text_lem'].str.upper())\n","print(X.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAfKCYpPN1iY"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","km = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)"]},{"cell_type":"markdown","metadata":{"id":"g3KWFAJVN1ia"},"source":["km.fit(X)\n","terms = vectorizer.get_feature_names()\n","order_centroids = km.cluster_centers_.argsort()[:,::-1]\n","for i in range(5):\n","    print(\"cluster %d:\" %i, end='')\n","    for ind in order_centroids[i,:10]:\n","        print(' %s' % terms[ind], end='')\n","    print()    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPHiFQkWN1ic"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","dist = 1 - cosine_similarity(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLfPh7M0N1ii"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","pos = pca.fit_transform(dist)\n","xs, ys = pos[:,0], pos[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuHRsk3nN1ik"},"outputs":[],"source":["\n","cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}#, 5: '#8A2BE2', 6: '#E9967A'}\n","\n","cluster_names = {0: 'cluster 1', \n","                 1: 'cluster 2', \n","                 2: 'cluster 3', \n","                 3: 'cluster 4', \n","                 4: 'cluster 5'}\n","                 #5: 'cluster 6',\n","                 #6: 'cluster 7'}\n","clusters = km.labels_.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLa4S67XN1im"},"outputs":[],"source":["\n","%matplotlib inline \n","\n","df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title='')) \n","\n","\n","groups = df.groupby('label')\n","\n","\n","\n","fig, ax = plt.subplots(figsize=(10, 4)) # set size\n","ax.margins(0.05) \n","\n","\n","for name, group in groups:\n","    ax.plot(group.x, group.y, marker='o', linestyle='', ms=10, \n","            label=cluster_names[name], color=cluster_colors[name],\n","            mec='none')\n","    ax.set_aspect('auto')\n","    ax.tick_params(\\\n","        axis= 'x',          \n","        which='both',     \n","        bottom='off',      \n","        top='off',         \n","        labelbottom='off')\n","    ax.tick_params(\\\n","        axis= 'y',        \n","        which='both',     \n","        left='off',      \n","        top='off',        \n","        labelleft='off')\n","    \n","ax.legend(numpoints=1)  \n","plt.title('Cluster plotting with ACP', bbox={'facecolor':'0.8', 'pad':0})\n","\n","for i in range(len(df)):\n","    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=5)  \n","\n","    \n","    \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZVQbwXJN1iq"},"outputs":[],"source":["import nltk\n","nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk.sentiment.util import *\n","from tablib.utils import *\n","from nltk import tokenize\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","tweets['sentiment_compound_polarity']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n","tweets['sentiment_neutral']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n","tweets['sentiment_negative']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n","tweets['sentiment_pos']=tweets.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n","tweets['sentiment_type']=''\n","tweets.loc[tweets.sentiment_compound_polarity\u003e0,'sentiment_type']='POSITIVE'\n","tweets.loc[tweets.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n","tweets.loc[tweets.sentiment_compound_polarity\u003c0,'sentiment_type']='NEGATIVE'\n","tweets.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHGeGN5YN1is"},"outputs":[],"source":["from  sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n","\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import cross_val_score\n","\n","\n","tweets_num_mod = tweets[tweets.select_dtypes(exclude=['object']).columns.values]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Glns0LjN1iw"},"outputs":[],"source":["\n","X,y=extract_feature(tweets_num_mod)\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n","ac=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGuSmAsEN1ix"},"outputs":[],"source":["from keras import layers\n","from keras.models import Sequential\n","\n","model = Sequential()\n","model.add(layers.Dense(20, input_dim=4, activation='relu'))\n","model.add(layers.Dense(10,  activation='tanh'))\n","model.add(layers.Dense(1024, activation='relu'))\n","\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', \n","              optimizer='adam')\n","model.summary()\n","classifier_nn = model.fit(X_train,y_train,\n","                    epochs=10,\n","                    verbose=True,\n","                    validation_data=(X_test, y_test),\n","                    batch_size=15)\n","ac.append(accuracy_score(classifier_nn,y_test,sample_weight=0.2)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIl0HNggN1i2"},"outputs":[],"source":["from sklearn.svm import SVC\n","clf=SVC(gamma='auto')\n","clf.fit(X_train, y_train)\n","y_pred=clf.predict(X_test)\n","ac.append(accuracy_score(y_pred,y_test)*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgNploL-N1i3"},"outputs":[],"source":["\n","def predict(s):\n","    model=SentimentIntensityAnalyzer()\n","    scores=model.polarity_scores(s)\n","    c=comp(scores)\n","    return c\n","def comp(scores):\n","    if(scores['neg']\u003escores['pos'] and scores['neg']\u003escores['neu'] ):\n","        return \"Predicted as depressed or sad tweet\"\n","\n","    elif(scores['pos']\u003escores['neg'] and scores['pos']\u003escores['neu'] ):\n","        return \"Predicted as happy tweet\"\n","    else:\n","        return \"Neutral tweet\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNICPK3TN1i4"},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib as plt\n","\n","plt.style.use('seaborn')\n","x=['CNN','SVM']\n"," \n","ax=sns.barplot(x,ac)\n","ax.set_title('Accuracy comparison')\n","ax.set_ylabel('Accuracy')\n","\n","print(\"the accuracy of {} is {} and {} is {}\".format(x[0],ac[0],x[1],ac[1]))\n","ax.set_ylim(50,100)\n","import pandas as pd\n","data={'Agorithms':x,\n","     \"accuracy\":ac}\n","df=pd.DataFrame(data)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9heom7RwN1i4"},"outputs":[],"source":["predict(\"i woke up early morning\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEOTkM2DN1i5"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqfLtPaDN1i5"},"outputs":[],"source":[]}],"metadata":{"_change_revision":12,"_is_fork":false,"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}